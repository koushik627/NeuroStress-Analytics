{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aQhxkj5fIyh",
        "outputId": "2cf831aa-9a65-4240-d7c2-befb85f594c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVqHLTbmfRI5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMMnxFy2fTL5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import pywt\n",
        "from scipy.stats import entropy\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import welch\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.metrics import structural_similarity as ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lQUJ5flfVbJ"
      },
      "outputs": [],
      "source": [
        "# Define the local folder path where you want to save the CSV files\n",
        "local_folder = '/content/local_data_folder'\n",
        "\n",
        "# Create the local folder if it doesn't exist\n",
        "os.makedirs(local_folder, exist_ok=True)\n",
        "\n",
        "# # to clear the contents of the directory\n",
        "# shutil.rmtree(local_folder, ignore_errors=True)\n",
        "# os.makedirs(local_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DWOf5KYfpmB"
      },
      "outputs": [],
      "source": [
        "# parent_folder_path = '/content/drive/MyDrive/Data'\n",
        "# # Create the new folder\n",
        "# feature_folder_path = os.path.join(parent_folder_path, test_name)\n",
        "# os.makedirs(feature_folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4R7mgNcf2Zj"
      },
      "outputs": [],
      "source": [
        "raw_data_folder = '/content/drive/MyDrive/Data/raw_data'\n",
        "# filtered_data_folder = '/content/drive/MyDrive/Data/filtered_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFAnOayDf-UD"
      },
      "outputs": [],
      "source": [
        "files = os.listdir(raw_data_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "def calculate_alpha(eeg_data_signal):\n",
        "\n",
        "    # Define the alpha band frequency range\n",
        "    alpha_low = 8.0  # Lower alpha band frequency (Hz)\n",
        "    alpha_high = 12.0  # Upper alpha band frequency (Hz)\n",
        "\n",
        "    # Define the sampling frequency (replace with your actual sampling frequency)\n",
        "    sampling_frequency = 128  # For example, 1000 Hz\n",
        "\n",
        "    # Design a bandpass filter to extract the alpha band\n",
        "    def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "        nyquist = 0.5 * fs\n",
        "        low = lowcut / nyquist\n",
        "        high = highcut / nyquist\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "\n",
        "    def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = lfilter(b, a, data, axis=1)\n",
        "        return y\n",
        "\n",
        "    # Apply the alpha bandpass filter to the EEG data\n",
        "    alpha_band_data = butter_bandpass_filter(eeg_data_signal, alpha_low, alpha_high, sampling_frequency)\n",
        "    return alpha_band_data\n",
        "\n",
        "    # Now, alpha_band_data contains the filtered EEG data in the alpha frequency range.\n"
      ],
      "metadata": {
        "id": "mTR5_ftje01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohJtSnDtTSgK"
      },
      "outputs": [],
      "source": [
        "# # loopnumber=0\n",
        "# test_names = [\"Mirror_image_sub\", \"Stroop_sub\", \"Arithmetic_sub\", \"Relax_sub\"]\n",
        "\n",
        "\n",
        "# beta_shape = (480, 32, 3200)\n",
        "# beta_aggregate = np.zeros(beta_shape)\n",
        "# alpha_aggregate = np.zeros(beta_shape)\n",
        "\n",
        "# # Loop through each .mat file in the folder\n",
        "# loopnumber = 0\n",
        "\n",
        "# for test_name in test_names:\n",
        "#     for file_name in files:\n",
        "#         if file_name.endswith('.mat')and file_name.startswith(test_name):\n",
        "#             data = loadmat(os.path.join(raw_data_folder, file_name))\n",
        "\n",
        "#             x = data['Data']\n",
        "#             df = pd.DataFrame(x)\n",
        "#         # Save the DataFrame as a CSV file in the local folder\n",
        "#             csv_file_name = os.path.splitext(file_name)[0] + '.csv'\n",
        "#             csv_file_path = os.path.join(local_folder, csv_file_name)\n",
        "#             df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "#             eeg_data = pd.read_csv(csv_file_path)\n",
        "#             eeg_array = eeg_data.values\n",
        "#         # print(eeg_array.size)\n",
        "#             eeg_array = eeg_array.T\n",
        "#             n_components = 4\n",
        "#             independent_components = np.zeros((n_components, eeg_array.shape[0], eeg_array.shape[1]))\n",
        "\n",
        "#             ica = FastICA(n_components=n_components, random_state=0)\n",
        "#             for i in range(eeg_array.shape[0]):\n",
        "#                 channel_data = eeg_array[i, :].reshape(-1, 1)\n",
        "#                 independent_channel = ica.fit_transform(channel_data)\n",
        "#                 independent_components[:, i, :] = independent_channel.T\n",
        "\n",
        "\n",
        "#             ica_data = independent_components[0, :, :]\n",
        "#             ica_data=ica_data.T # ica_data.shape\n",
        "\n",
        "\n",
        "#             rows_indices = [i for i in range(32)]\n",
        "\n",
        "#             stress = ica_data[rows_indices, :]\n",
        "\n",
        "#             eeg_data = stress\n",
        "\n",
        "#             alpha_array = calculate_alpha(eeg_data)\n",
        "\n",
        "#             sampling_rate = 128\n",
        "#             central_frequency = 22\n",
        "\n",
        "#             sigma = 1 / (2 * np.pi * central_frequency)\n",
        "#             scales = np.arange(13, 31)\n",
        "\n",
        "\n",
        "#             morlet_coefficients = []\n",
        "#             for channel in eeg_data:\n",
        "#                 coefficients, frequencies = pywt.cwt(channel, scales, 'morl', sampling_period=1/sampling_rate)\n",
        "#                 morlet_coefficients.append(coefficients)\n",
        "\n",
        "#             beta_band_power_array = np.abs(np.array(morlet_coefficients)) ** 2 #This shape is 32,18,3200\n",
        "#             collapsed_beta_band_power_array = np.mean(beta_band_power_array, axis = 1)\n",
        "#             print('loopnumber : ', loopnumber, ' , here the shape is ', test_name, collapsed_beta_band_power_array.shape)\n",
        "#             beta_aggregate[loopnumber] = collapsed_beta_band_power_array\n",
        "#             alpha_aggregate[loopnumber] = alpha_array\n",
        "#             loopnumber = loopnumber + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save('/content/drive/My Drive/32_chnl/raw_data_Beta.npy', beta_aggregate)\n",
        "# np.save('/content/drive/My Drive/32_chnl/raw_data_Alpha.npy', alpha_aggregate)"
      ],
      "metadata": {
        "id": "sFW4vH2XDFS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_aggregate = np.load('/content/drive/My Drive/32_chnl/raw_data_Beta.npy')\n",
        "alpha_aggregate = np.load('/content/drive/My Drive/32_chnl/raw_data_Alpha.npy')"
      ],
      "metadata": {
        "id": "R9Pl0dP-DMxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rweFEFbZ_LAk"
      },
      "outputs": [],
      "source": [
        "# Split the original array into four smaller arrays\n",
        "taskwise_beta_split = np.split(beta_aggregate, 4, axis=0)\n",
        "\n",
        "# Each split_array will have shape (120, 32, 3200)\n",
        "beta_Mirror_task, beta_stroop_task, beta_arthimetic_task, beta_rest_task = taskwise_beta_split\n",
        "# print(beta_Mirror_task.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the original array into four smaller arrays\n",
        "taskwise_alpha_split = np.split(alpha_aggregate, 4, axis=0)\n",
        "\n",
        "# Each split_array will have shape (120, 32, 3200)\n",
        "alpha_Mirror_task, alpha_stroop_task, alpha_arthimetic_task, alpha_rest_task = taskwise_alpha_split\n",
        "# print(beta_Mirror_task.shape)"
      ],
      "metadata": {
        "id": "D2vSX4XsomGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV4-TX21yxC8"
      },
      "outputs": [],
      "source": [
        "# def calculate_entropy(input_array, epsilon = 1e-10):\n",
        "#     # Calculate entropy for a 3D array of shape (120, 1, 3200) and returns a 2D array of shape (120, 1) with entropy values.\n",
        "#     # Reshape the array to (120, 3200) by removing the middle dimension\n",
        "#     reshaped_array = input_array[:, 0, :]\n",
        "\n",
        "#     # Initialize an array to store the entropy values\n",
        "#     entropy_array = np.zeros((120, 1))\n",
        "\n",
        "#     # Calculate the entropy for each signal in the reshaped array\n",
        "#     for i in range(120):\n",
        "#         signal = reshaped_array[i, :]\n",
        "#         signal_with_epsilon = signal + epsilon\n",
        "#         entropy_value = entropy(signal_with_epsilon)\n",
        "#         entropy_array[i, 0] = entropy_value\n",
        "\n",
        "#     return entropy_array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(input_array):\n",
        "    entropy_values = np.zeros((120, 1))\n",
        "    i = 0\n",
        "    for data in input_array:\n",
        "        data_T = data.reshape(3200,)\n",
        "\n",
        "        num_bins = int(np.sqrt(len(data_T)))  # Adjust the number of bins as needed\n",
        "        # num_bins = 20  # Adjust the number of bins as needed\n",
        "\n",
        "        # Calculate the histogram and probabilities\n",
        "        hist, bin_edges = np.histogram(data_T, bins=num_bins, density=True)\n",
        "        probabilities = hist / np.sum(hist)\n",
        "\n",
        "        # Calculate entropy using scipy's entropy function\n",
        "        data_entropy = entropy(probabilities, base=2)\n",
        "        # print(\"data entropy is\",data_entropy,\"\\n\")\n",
        "        entropy_values[i] = data_entropy\n",
        "        i += 1\n",
        "    return entropy_values"
      ],
      "metadata": {
        "id": "wJi1U05vGnZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG1vHAxUzvmV"
      },
      "outputs": [],
      "source": [
        "def calculate_psd(input_array, sampling_rate):\n",
        "    # Calculate PSD for a 3D array of shape (120, 1, 3200) and returns a 2D array of shape (120, 1) with PSD values.\n",
        "    # Reshape the array to (120, 3200) by removing the middle dimension\n",
        "    reshaped_array = input_array[:, 0, :]\n",
        "\n",
        "    # Initialize an array to store the PSD values\n",
        "    psd_array = np.zeros((120, 1))\n",
        "\n",
        "    # Calculate PSD for each signal in the reshaped array\n",
        "    for i in range(120):\n",
        "        signal = reshaped_array[i, :]\n",
        "        f, pxx = welch(signal, fs=sampling_rate, nperseg=256)  # Adjust nperseg as needed\n",
        "        psd_value = np.mean(pxx)  # You can use np.sum(pxx) for total power instead of mean\n",
        "        psd_array[i, 0] = psd_value\n",
        "\n",
        "    return psd_array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_matrix_1 = np.empty((0, 96))\n",
        "\n",
        "for task in taskwise_beta_split:\n",
        "    each_task_beta_dimensions = (120, 32, 3200)\n",
        "    channels = []\n",
        "\n",
        "    for i in range(each_task_beta_dimensions[1]):\n",
        "        reshaped_array = task[:, i:i + 1, :]\n",
        "        channels.append(reshaped_array)\n",
        "\n",
        "\n",
        "    featurerow = np.mean(channels[0], axis=2)\n",
        "\n",
        "    for i in range(31):\n",
        "        channel = channels[i+1]\n",
        "        mean_array = np.mean(channel, axis=2)\n",
        "        featurerow = np.hstack((featurerow, mean_array))\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate entropy and add to the feature row\n",
        "        entropy_array = calculate_entropy(channel)\n",
        "        featurerow = np.hstack((featurerow, entropy_array))\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate PSD and add to the feature row\n",
        "        psd_array = calculate_psd(channel, 128)\n",
        "        featurerow = np.hstack((featurerow, psd_array))\n",
        "\n",
        "    # Append the feature row to the feature matrix\n",
        "    feature_matrix_1 = np.vstack((feature_matrix_1, featurerow))\n",
        "\n",
        "# Ensure feature_matrix dimensions are correct (number of rows, 96 columns)\n",
        "print(feature_matrix_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoRk4CRT1A68",
        "outputId": "2332c43e-1c42-458a-b06c-b15be90af1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix_2 = np.empty((0, 96))\n",
        "for task in taskwise_alpha_split:\n",
        "    each_task_alpha_dimensions = (120, 32, 3200)\n",
        "    channels = []\n",
        "\n",
        "    for i in range(each_task_alpha_dimensions[1]):\n",
        "        reshaped_array = task[:, i:i + 1, :]\n",
        "        channels.append(reshaped_array)\n",
        "\n",
        "\n",
        "    featurerow = np.mean(channels[0], axis=2)\n",
        "\n",
        "    for i in range(31):\n",
        "        channel = channels[i+1]\n",
        "        mean_array = np.mean(channel, axis=2)\n",
        "        featurerow = np.hstack((featurerow, mean_array))\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate entropy and add to the feature row\n",
        "        entropy_array = calculate_entropy(channel)\n",
        "        featurerow = np.hstack((featurerow, entropy_array))\n",
        "\n",
        "    # print(featurerow.min())\n",
        "\n",
        "    for i in range(32):\n",
        "        channel = channels[i]\n",
        "        # Calculate PSD and add to the feature row\n",
        "        psd_array = calculate_psd(channel, 128)\n",
        "        featurerow = np.hstack((featurerow, psd_array))\n",
        "\n",
        "    # Append the feature row to the feature matrix\n",
        "    feature_matrix_2 = np.vstack((feature_matrix_2, featurerow))\n",
        "\n",
        "# Ensure feature_matrix dimensions are correct (number of rows, 96 columns)\n",
        "print(feature_matrix_2.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGrAxbxOpjee",
        "outputId": "3f797344-6203-4b54-ad22-890d5e83e06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix = np.hstack((feature_matrix_1, feature_matrix_2))"
      ],
      "metadata": {
        "id": "CobPlRtFqcSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYhqeMSfqnQ_",
        "outputId": "2176ebe3-519a-4baa-9fcf-4c027f427660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(480, 192)"
            ]
          },
          "metadata": {},
          "execution_count": 433
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'feature_matrix' is your NumPy array\n",
        "df = pd.DataFrame(feature_matrix)\n",
        "df.to_csv('/content/drive/My Drive/32_chnl/raw_feature_matrix.csv', index=False)"
      ],
      "metadata": {
        "id": "y5ciCsCoriKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECHt2YaERNz1"
      },
      "outputs": [],
      "source": [
        "# # this cell is only to see how hstack and vstack are working\n",
        "# feature_matrix_dum = np.empty((0, 3))\n",
        "# m1 = np.array([[1],[2],[3],[4]])\n",
        "# m2 = np.array([[5],[6],[7],[8]])\n",
        "# m3 = np.array([[9],[10],[11],[12]])\n",
        "# row1=np.hstack((m1, m2, m3))\n",
        "# m11 = np.array([[101],[102],[103],[104]])\n",
        "# m22 = np.array([[105],[106],[107],[108]])\n",
        "# m33 = np.array([[109],[110],[111],[112]])\n",
        "# row2=np.hstack((m11, m22, m33))\n",
        "\n",
        "# print(feature_matrix_dum.shape)\n",
        "# feature_matrix_dum = np.vstack((feature_matrix_dum, row1))\n",
        "# print(feature_matrix_dum.shape)\n",
        "# print(feature_matrix_dum)\n",
        "# feature_matrix_dum = np.vstack((feature_matrix_dum, row2))\n",
        "# print(feature_matrix_dum.shape)\n",
        "# print(feature_matrix_dum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjLFJnuoh9gi"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('/content/drive/My Drive/32_chnl/raw_feature_matrix.csv')\n",
        "y = np.concatenate([np.ones(360), np.zeros(120)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEYh9fBUl8Sq",
        "outputId": "a430d3ec-3d3f-40a1-e6a6-fce1f117eeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 69.79%\n",
            "SVM Accuracy: 68.75%\n",
            "Random Forest Accuracy: 70.83%\n",
            "Gradient Boosting Accuracy: 70.83%\n",
            "K-NN Accuracy: 71.88%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Assuming X is your feature matrix and y is your target labels\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "logistic_regression_model = LogisticRegression()\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "logistic_regression_predictions = logistic_regression_model.predict(X_test)\n",
        "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)\n",
        "print(f\"Logistic Regression Accuracy: {logistic_regression_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(kernel='linear')  # You can choose different kernel functions\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(f\"SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Random Forest\n",
        "random_forest_model = RandomForestClassifier()\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "random_forest_predictions = random_forest_model.predict(X_test)\n",
        "random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)\n",
        "print(f\"Random Forest Accuracy: {random_forest_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gradient_boosting_model = GradientBoostingClassifier()\n",
        "gradient_boosting_model.fit(X_train, y_train)\n",
        "gradient_boosting_predictions = gradient_boosting_model.predict(X_test)\n",
        "gradient_boosting_accuracy = accuracy_score(y_test, gradient_boosting_predictions)\n",
        "print(f\"Gradient Boosting Accuracy: {gradient_boosting_accuracy * 100:.2f}%\")\n",
        "\n",
        "# K-Nearest Neighbors (K-NN)\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_predictions = knn_model.predict(X_test)\n",
        "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "print(f\"K-NN Accuracy: {knn_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title _\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# # Create classifiers for Random Forest, SVM, KNN, Logistic Regression, and Gradient Boosting\n",
        "# random_forest_model = RandomForestClassifier()\n",
        "# svm_model = SVC()\n",
        "# knn_model = KNeighborsClassifier()\n",
        "# logistic_regression_model = LogisticRegression()\n",
        "# gradient_boosting_model = GradientBoostingClassifier()\n",
        "\n",
        "# # Create a list of models\n",
        "# models = [logistic_regression_model, svm_model, random_forest_model, gradient_boosting_model, knn_model]\n",
        "\n",
        "# # Perform k-fold cross-validation (e.g., k=5) for each model\n",
        "# k_fold = 5\n",
        "# for model in models:\n",
        "#     cross_val_scores = cross_val_score(model, X_train, y_train, cv=k_fold)\n",
        "\n",
        "#     # Print the cross-validation scores\n",
        "#     model_name = model.__class__.__name__\n",
        "#     # print(f\"{model_name} Cross-Validation Scores: {cross_val_scores}\")\n",
        "#     # print(f\"Mean {model_name} Cross-Validation Accuracy: {cross_val_scores.mean() * 100:.2f}%\")\n",
        "\n",
        "#     # Fit the model on the entire training set\n",
        "#     model.fit(X_train, y_train)\n",
        "\n",
        "#     # Predict on the test set\n",
        "#     model_predictions = model.predict(X_test)\n",
        "\n",
        "#     # Calculate accuracy on the test set\n",
        "#     model_accuracy = accuracy_score(y_test, model_predictions)\n",
        "#     print(f\"{model_name} Accuracy on Test Set: {model_accuracy * 100:.2f}%\")\n",
        "#     # print()\n"
      ],
      "metadata": {
        "id": "VqE8bUDFeQLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channels = [\"CZ\", \"FZ\", \"Fp1\", \"F7\", \"F3\", \"FC1\", \"C3\", \"FC5\", \"FT9\", \"T7\", \"CP5\", \"CP1\", \"P3\", \"P7\", \"PO9\", \"O1\", \"PZ\", \"OZ\", \"O2\", \"PO10\", \"P8\", \"P4\", \"CP2\", \"CP6\", \"T8\", \"FT10\", \"FC6\", \"C4\", \"FC2\", \"F4\", \"F8\", \"Fp2\"]\n",
        "\n",
        "# Create a mapping dictionary from numbers to channel names\n",
        "channel_mapping = {i+1: channel for i, channel in enumerate(channels)}\n",
        "\n",
        "# Test the mapping\n",
        "print(channel_mapping)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZbz8DDy6Law",
        "outputId": "5ebb9afc-5ba7-42ef-a083-f620d8df02e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'CZ', 2: 'FZ', 3: 'Fp1', 4: 'F7', 5: 'F3', 6: 'FC1', 7: 'C3', 8: 'FC5', 9: 'FT9', 10: 'T7', 11: 'CP5', 12: 'CP1', 13: 'P3', 14: 'P7', 15: 'PO9', 16: 'O1', 17: 'PZ', 18: 'OZ', 19: 'O2', 20: 'PO10', 21: 'P8', 22: 'P4', 23: 'CP2', 24: 'CP6', 25: 'T8', 26: 'FT10', 27: 'FC6', 28: 'C4', 29: 'FC2', 30: 'F4', 31: 'F8', 32: 'Fp2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming you have a feature matrix X of shape (480, 96)\n",
        "# where each row represents a sample (EEG recording) and each column is a feature\n",
        "\n",
        "# Define your target variable (stress levels), let's call it y\n",
        "# Make sure y is of shape (480,)\n",
        "y = np.concatenate([np.ones(360), np.zeros(120)])\n",
        "\n",
        "# Scale your feature matrix\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Calculate mutual information scores for each channel\n",
        "mi_scores = mutual_info_classif(X_scaled, y)\n",
        "\n",
        "# Create a list of (channel index, channel name, mutual information score) tuples\n",
        "channel_scores = [(i, channel_mapping[i], score) for i, score in enumerate(mi_scores) if i in channel_mapping]\n",
        "\n",
        "# Sort the list by mutual information score in descending order\n",
        "channel_scores.sort(key=lambda x: x[2], reverse=True)\n",
        "z=1\n",
        "# Print the channels with their mutual information scores in decreasing order\n",
        "for i, channel, score in channel_scores:\n",
        "    print(f\"{z}\\t Channel {i} - {channel}: Mutual Information Score = {score}\")\n",
        "    z += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzry8tkP2iI",
        "outputId": "0198f5a0-6905-4cb0-e6c0-68e3cf811af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t Channel 27 - FC6: Mutual Information Score = 0.0566038026139597\n",
            "2\t Channel 30 - F4: Mutual Information Score = 0.0466492246072423\n",
            "3\t Channel 10 - T7: Mutual Information Score = 0.03740851384866817\n",
            "4\t Channel 29 - FC2: Mutual Information Score = 0.035322690433829296\n",
            "5\t Channel 32 - Fp2: Mutual Information Score = 0.030122597233637416\n",
            "6\t Channel 14 - P7: Mutual Information Score = 0.027592471474403357\n",
            "7\t Channel 17 - PZ: Mutual Information Score = 0.02707825969120048\n",
            "8\t Channel 15 - PO9: Mutual Information Score = 0.025442390378667135\n",
            "9\t Channel 12 - CP1: Mutual Information Score = 0.02527606481843958\n",
            "10\t Channel 25 - T8: Mutual Information Score = 0.02354290814387472\n",
            "11\t Channel 16 - O1: Mutual Information Score = 0.020351003724386807\n",
            "12\t Channel 19 - O2: Mutual Information Score = 0.020090487507449906\n",
            "13\t Channel 26 - FT10: Mutual Information Score = 0.017653252573319733\n",
            "14\t Channel 20 - PO10: Mutual Information Score = 0.01684836954059521\n",
            "15\t Channel 4 - F7: Mutual Information Score = 0.016263948221025837\n",
            "16\t Channel 3 - Fp1: Mutual Information Score = 0.011754595224120479\n",
            "17\t Channel 1 - CZ: Mutual Information Score = 0.011569733229833234\n",
            "18\t Channel 5 - F3: Mutual Information Score = 0.009410422990851375\n",
            "19\t Channel 8 - FC5: Mutual Information Score = 0.00856700074372152\n",
            "20\t Channel 21 - P8: Mutual Information Score = 0.004881027361824275\n",
            "21\t Channel 13 - P3: Mutual Information Score = 0.004120956853472801\n",
            "22\t Channel 18 - OZ: Mutual Information Score = 0.002862115693557099\n",
            "23\t Channel 7 - C3: Mutual Information Score = 0.001196383593226047\n",
            "24\t Channel 2 - FZ: Mutual Information Score = 0.0\n",
            "25\t Channel 6 - FC1: Mutual Information Score = 0.0\n",
            "26\t Channel 9 - FT9: Mutual Information Score = 0.0\n",
            "27\t Channel 11 - CP5: Mutual Information Score = 0.0\n",
            "28\t Channel 22 - P4: Mutual Information Score = 0.0\n",
            "29\t Channel 23 - CP2: Mutual Information Score = 0.0\n",
            "30\t Channel 24 - CP6: Mutual Information Score = 0.0\n",
            "31\t Channel 28 - C4: Mutual Information Score = 0.0\n",
            "32\t Channel 31 - F8: Mutual Information Score = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "channels = [27, 30]\n",
        "for channel in channels:\n",
        "  selected_columns = [channel+63]\n",
        "  psd = [0, 0, 0, 0]\n",
        "  for i in range(4):\n",
        "    selected_rows = [(i*120) + a for a in range(120)]\n",
        "    psd[i] = np.mean(feature_matrix[selected_rows][:, selected_columns])\n",
        "\n",
        "  # for i in range(4):\n",
        "  #   psd[i] /= psd[3]\n",
        "\n",
        "  print(psd)\n",
        "  # MSAR : we must get A > M > S > R\n",
        "  # in these 5 cases we are getting A > M > S > R\n",
        "  # in these 5 cases we are getting M >= A > S > R so A > M > S > R\n",
        "  # so we can conclude that from the 2 most dominant channels with Mutual Information Score >= 0.045 show that A > M > S > R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMqa6INSbAAr",
        "outputId": "6f561270-f8e7-45bf-b375-59f4637fec96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.9391661335766255e-05, 1.5457594697470777e-05, 2.358307581634193e-05, 1.1373670739332619e-05]\n",
            "[1.8591146718272506e-05, 1.786607201798924e-05, 1.7909278085325388e-05, 1.650308811556642e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title _\n",
        "psd = [0, 0, 0, 0]\n",
        "# channels = [0, 4, 5, 7, 8, 14, 24, 25, 26, 29]\n",
        "# selected_columns = [64, 68, 69, 71, 72, 78, 88, 89, 90, 93]\n",
        "\n",
        "channels = [27, 30]#, 10, 29, 14, 17]\n",
        "selected_columns = [i + 63 for i in channels]\n",
        "\n",
        "for i in range(4):\n",
        "  selected_rows = [(i*120) + a for a in range(120)]\n",
        "  psd[i] = np.mean(feature_matrix[selected_rows][:, selected_columns])\n",
        "\n",
        "for i in range(4):\n",
        "  psd[i] /= psd[3]\n",
        "\n",
        "print(psd)\n",
        "# MSAR :  we are getting S > A > M > R, but we must get A > M > S > R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltwVNYAbXgiQ",
        "outputId": "088cb3dd-f685-4bc4-e072-0742aebc5b8e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.3625259755534203, 1.1953924374391085, 1.4884210218856018, 1.0]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}